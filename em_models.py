# -*- coding: utf-8 -*-
"""EM_Models

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Iy-BryP6QMqYxzO7-HLBsn6-Vhqhv6lo
"""

!pip show scikit-learn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from joblib import load, dump
# Linear Regression Dependencies
from sklearn.linear_model import LinearRegression
# Support Vector Regression Dependencies
from sklearn.svm import SVR
# Neural Network Dependencies
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPRegressor
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
# XGBOOST Dependencies
import xgboost as xgb
# Random Forest Regressor Dependencies
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import confusion_matrix, roc_curve, auc
import seaborn as sns

df = pd.read_excel('Final Data Set - Altered.xlsx')

df

df.dtypes

df.mean()

# Data Preprocessing
df = df.fillna(df.mean())

# Assuming your dataframe is named 'df' and the target variable is 'y'
# Replace 'your_feature_columns' with the actual feature columns in your dataframe
X = df[['Matric percentage', 'Intermediate percentage', 'SGPA in BS First semester', 'SGPA in BS Second semester', 'SGPA in BS Third semester', 'SGPA in BS Fourth semester']]
y = df['SGPA in BS Fifth semester']

print(len(X))
print(len(y))

# Split the data into training and testing sets (70% training, 30% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

X.shape

type(X)

type(y)

np.ma.ravel(X).size

np.ma.ravel(y).size

"""## Linear Regression"""

X_test[:1].dtypes

d = {
    "Matric percentage": [80],
    "Intermediate percentage": [70],
    "SGPA in BS First semester": [1],
    "SGPA in BS Second semester": [2],
    "SGPA in BS Third semester": [3],
    "SGPA in BS Fourth semester": [2.5]
}

ddf = pd.DataFrame(d)

ddf.dtypes

a = model.predict(ddf)

a[0]

X_test

# Initialize the linear regression model
model = LinearRegression()

# Train the model
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Save the model
dump(model, 'linear_regression.joblib')

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print the metrics
print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')

# Optionally, you can visualize the predictions against the actual values
plt.scatter(y_test, y_pred)
# plt.scatter(X_test['SGPA in BS First semester'], y_test, color='black')
# plt.scatter(X_test['SGPA in BS First semester'], y_pred, color='blue', linewidth=3)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Linear Regression: Actual vs Predicted')
plt.show()

"""## Support Vector Regression (SVR)"""

# Initialize the SVR model
svr = SVR(kernel='linear')  # You can choose different kernels like 'linear', 'poly', or 'rbf'

# Train the model
svr.fit(X_train, y_train)

# Make predictions on the test set
y_pred = svr.predict(X_test)

# Save the model
dump(svr, 'svr.joblib')

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')

# Optionally, you can plot the predicted vs actual values
plt.scatter(y_test, y_pred)
# plt.scatter(X_test['SGPA in BS First semester'], y_test, color='black')
# plt.scatter(X_test['SGPA in BS First semester'], y_pred, color='blue', linewidth=3)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('SVR: Actual vs Predicted')
plt.show()

"""## Neural Network"""

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Using scikit-learn's MLPRegressor as an example
mlp_regressor = MLPRegressor(solver='adam', hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)
mlp_regressor.fit(X_train_scaled, y_train)

# Make predictions
y_pred = mlp_regressor.predict(X_test_scaled)

# Evaluate using Mean Squared Error
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

# Plotting the training and validation accuracy over epochs
plt.figure(figsize=(10, 6))
plt.plot(mlp_regressor.loss_curve_, label='Training Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training Loss over Epochs')
plt.legend()
plt.show()

# Now, let's create a simple neural network using TensorFlow for demonstration
model = Sequential()
model.add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='linear'))

model.compile(loss='mean_squared_error', optimizer='adam')

# Train the model and record training history
history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=5, validation_data=(X_test_scaled, y_test))

# Make predictions using the trained model
y_pred_tf = model.predict(X_test_scaled)

# Save the model
# dump(model, 'neural_network.joblib')
model.save('neural_network.h5')

# Evaluate using Mean Squared Error
mse_tf = mean_squared_error(y_test, y_pred_tf)
r2_tf = r2_score(y_test, y_pred)
print(f'TensorFlow Model Mean Squared Error: {mse_tf}')
print(f'TensorFlow Model R-Squared: {r2_tf}')

# Plotting the training and validation accuracy over epochs using TensorFlow model
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss over Epochs')
plt.legend()
plt.show()

plt.scatter(y_test, y_pred_tf)
# plt.scatter(X_test['SGPA in BS First semester'], y_test, color='black')
# plt.scatter(X_test['SGPA in BS First semester'], y_pred, color='blue', linewidth=3)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Neural Network: Actual vs Predicted')
plt.show()

# Instantiate an XGBoost regressor
xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,
                max_depth = 50, alpha = 10, n_estimators = 10)

# Fit the model to the training set
xg_reg.fit(X_train, y_train)

# Make predictions on the test set
y_pred = xg_reg.predict(X_test)

# Save the model
dump(xg_reg, 'xgboost.joblib')

# Evaluate the model using Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred)
r_squared = r2_score(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r_squared}')

plt.scatter(y_test, y_pred)
# plt.scatter(X_test['SGPA in BS First semester'], y_test, color='black')
# plt.scatter(X_test['SGPA in BS First semester'], y_pred, color='blue', linewidth=3)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('XGBOOST: Actual vs Predicted')
plt.show()

"""## Random Forest Regressor"""

# Step 3: Train the Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Step 4: Make predictions on the test set
y_pred = rf_model.predict(X_test)

# Save the model
dump(rf_model, 'random_forest_regressor.joblib')

# Step 5: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')

# Scatter plot of true values vs predicted values
plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_test, y=y_pred, color='blue')
plt.title('True Values vs Predicted Values')
plt.xlabel('True Values')
plt.ylabel('Predicted Values')
plt.show()